# Technical Architecture

## ğŸ—ï¸ System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GitHub Actions (Scheduler)               â”‚
â”‚                    Runs every X minutes                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      watcher.py                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  1. Load state.json (seen URLs)                       â”‚  â”‚
â”‚  â”‚  2. Scrape target pages (async)                       â”‚  â”‚
â”‚  â”‚  3. Extract articles & titles                         â”‚  â”‚
â”‚  â”‚  4. Deduplicate by URL                                â”‚  â”‚
â”‚  â”‚  5. Find new articles (not in state)                  â”‚  â”‚
â”‚  â”‚  6. Fetch article dates (if needed)                   â”‚  â”‚
â”‚  â”‚  7. Send Discord webhooks (embeds)                    â”‚  â”‚
â”‚  â”‚  8. Update state.json                                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼               â–¼               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Discord â”‚   â”‚  SIP     â”‚   â”‚ state.   â”‚
    â”‚ Webhook â”‚   â”‚  Elfak   â”‚   â”‚ json     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                               â”‚  Git Commit  â”‚
                               â”‚  & Push      â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Component Details

### 1. **watcher.py** (Main Scraper)

**Dependencies:**

- `httpx`: Async HTTP client with HTTP/2 support
- `selectolax`: Fast HTML parser (Rust-based, uses Modest engine)

**Key Functions:**

| Function                             | Purpose                                    | Performance            |
| ------------------------------------ | ------------------------------------------ | ---------------------- |
| `fetch_page()`                       | Async page fetching with retries           | ~100-300ms per page    |
| `extract_article_urls()`             | Extract all /article/ links from list page | ~5-10ms per page       |
| `parse_article_page()`               | Parse full article (title, date, content)  | ~200-400ms per article |
| `parse_serbian_date()`               | Parse Serbian date format with time        | <1ms                   |
| `is_article_recent()`                | Filter articles by CUTOFF_DATE             | <1ms                   |
| `normalize_content_from_container()` | Extract formatted content with Markdown    | ~10-20ms per article   |
| `send_discord_message()`             | Send rich embed to Discord webhook         | ~50-150ms per webhook  |

**Flow (Two-Phase Architecture):**

```
Phase 1: Collect URLs
1. Load state â”€â”€â†’ 2. Fetch all 15 category pages â”€â”€â†’ 3. Extract article URLs
                   (async parallel)                        â†“
                                                     4. Group by category
                                                            â†“
                                                     5. Find new URLs

Phase 2: Parse & Send
6. For each new URL:
   â”œâ”€ Fetch full article page
   â”œâ”€ Parse title, date, content, image
   â”œâ”€ Parse Serbian date (including time)
   â””â”€ Filter by CUTOFF_DATE (Dec 1, 2025)
        â†“
7. Deduplicate by title + content
   (not just URL)
        â†“
8. Sort by publication date
   (oldest first)
        â†“
9. Send to Discord in chronological order
   â”œâ”€ Rich embeds with Markdown
   â”œâ”€ Accurate timestamp from article
   â”œâ”€ Bot name: "Elfak SIP"
   â””â”€ Avatar: Elfak logo (if available)
        â†“
10. Update state.json
```

### 2. **Article Extraction Strategy**

The scraper uses a **multi-level fallback** approach:

```python
1. Find all <a href="/article/...">
   â†“
2. Attempt to extract title:
   â”œâ”€ Check link text (not "OpÅ¡irnije")
   â”œâ”€ Search parent for <h1>-<h6>
   â”œâ”€ Look for class*="title" or class*="naslov"
   â”œâ”€ Traverse up DOM tree for headings
   â””â”€ Fallback: Use URL slug
   â†“
3. Create Article object with URL (unique key)
```

This handles various HTML structures on the site.

### 3. **Date Extraction & Filtering**

**Serbian Date Parsing:**

- Format: `"ĞŸĞ¾Ğ½, 24. ĞĞ¾Ğ², 2025. Ñƒ 13:52"`
- Extracts: day, month (Cyrillic), year, hour, minute
- Timezone: UTC
- Returns: `datetime` object

**Date Filtering:**

```python
CUTOFF_DATE = datetime(2025, 12, 1, tzinfo=timezone.utc)

# Only articles after Dec 1, 2025 are posted to Discord
if article_date < CUTOFF_DATE:
    skip article
```

**Supported Month Names:**

| Serbian   | Short | Number |
| --------- | ----- | ------ |
| Ñ˜Ğ°Ğ½ÑƒĞ°Ñ€    | Ñ˜Ğ°Ğ½   | 1      |
| Ñ„ĞµĞ±Ñ€ÑƒĞ°Ñ€   | Ñ„ĞµĞ±   | 2      |
| Ğ¼Ğ°Ñ€Ñ‚      | Ğ¼Ğ°Ñ€   | 3      |
| Ğ°Ğ¿Ñ€Ğ¸Ğ»     | Ğ°Ğ¿Ñ€   | 4      |
| Ğ¼Ğ°Ñ˜       | Ğ¼Ğ°Ñ˜   | 5      |
| Ñ˜ÑƒĞ½       | Ñ˜ÑƒĞ½   | 6      |
| Ñ˜ÑƒĞ»       | Ñ˜ÑƒĞ»   | 7      |
| Ğ°Ğ²Ğ³ÑƒÑÑ‚    | Ğ°Ğ²Ğ³   | 8      |
| ÑĞµĞ¿Ñ‚ĞµĞ¼Ğ±Ğ°Ñ€ | ÑĞµĞ¿   | 9      |
| Ğ¾ĞºÑ‚Ğ¾Ğ±Ğ°Ñ€   | Ğ¾ĞºÑ‚   | 10     |
| Ğ½Ğ¾Ğ²ĞµĞ¼Ğ±Ğ°Ñ€  | Ğ½Ğ¾Ğ²   | 11     |
| Ğ´ĞµÑ†ĞµĞ¼Ğ±Ğ°Ñ€  | Ğ´ĞµÑ†   | 12     |

### 4. **Deduplication Strategy**

**Two-level deduplication:**

1. **URL-based** (state.json):

   ```python
   if url in seen_urls:
       skip
   ```

2. **Content-based** (before posting):
   ```python
   content_hash = (title.lower(), content[:500].lower())
   if content_hash in seen_content:
       skip duplicate
   ```

This prevents posting:

- Same article re-posted under different URL
- Duplicate announcements with identical content
- Cross-posted articles from different categories

### 5. **Chronological Sorting**

Before sending to Discord, all articles are sorted by publication date:

```python
articles.sort(key=lambda a: parse_serbian_date(a.date))
```

**Why oldest-first?**

- More natural reading order
- Historical context preserved
- Better for following multi-day announcements

**Two-phase approach:**

```
Phase 1: List Page
â”œâ”€ Try to extract from card/listing
â””â”€ If not found â†’ Phase 2

Phase 2: Article Page (lazy fetch)
â”œâ”€ Check meta tags: article:published_time
â”œâ”€ Check <time> tags with datetime attribute
â”œâ”€ Search for class*="date", class*="datum"
â””â”€ Fallback: null (date not shown in embed)
```

**Rate Limiting:**

- Sleep 0.7s between article page fetches
- Prevents overwhelming the server
- Complies with polite scraping guidelines

### 6. **State Management**

**state.json structure:**

```json
{
  "seen_urls": [
    "https://sip.elfak.ni.ac.rs/article/12345",
    "https://sip.elfak.ni.ac.rs/article/12346",
    ...
  ],
  "last_checked": "2026-01-04T10:15:00.000Z"
}
```

**Operations:**

- **Load**: `O(n)` - read JSON, convert to set
- **Check**: `O(1)` - set membership test
- **Update**: `O(n)` - add new URLs to set
- **Save**: `O(n)` - write JSON with sorted list (for clean diffs)

**Important:** URLs are tracked even if article is skipped (e.g., duplicate content or old date)

### 7. **Discord Integration**

**Webhook Payload:**

```json
{
  "embeds": [
    {
      "author": {
        "name": "SIP Elfak",
        "url": "https://sip.elfak.ni.ac.rs"
      },
      "title": "Article title (max 256 chars)",
      "url": "https://...",
      "description": "Full article content with Markdown formatting",
      "fields": [
        {
          "name": "ğŸ“… ĞĞ±Ñ˜Ğ°Ğ²Ñ™ĞµĞ½Ğ¾",
          "value": "Ğ¡Ñ€Ğµ, 31. Ğ”ĞµÑ†, 2025. Ñƒ 12:48",
          "inline": true
        },
        {
          "name": "ğŸ“‚ ĞšĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ˜Ğ°",
          "value": "ĞÑÑ‚Ğ°Ğ»Ğ¾",
          "inline": true
        }
      ],
      "color": 43263, // #0099FF (brighter blue)
      "timestamp": "2025-12-31T12:48:00+00:00", // Actual article date/time
      "thumbnail": { "url": "https://..." }, // Or "image" for image-only posts
      "footer": { "text": "SIP Elfak Bot" }
    }
  ],
  "username": "Elfak SIP",
  "avatar_url": "https://sip.elfak.ni.ac.rs/images/logos/logo.svg"
}
```

**Key Features:**

- **Timestamp**: Uses actual article publication date/time (not current time)
- **Content**: Full article with Markdown ([links], **bold**, bullet lists)
- **Images**: Thumbnail for text posts, full image for image-only posts
- **Bot Identity**: Custom name and avatar
- **Fields**: Category and date shown separately for clarity
  "username": "SIP Elfak Notifier"
  }

````

**Rate Limits:**

- Discord: 30 req/min per webhook
- Bot: 2.0s sleep between sends (DISCORD_SEND_DELAY)
- Bot: 0.5s sleep between fetches (RATE_LIMIT_SLEEP)
- Typical: 1-10 articles per run
- **Well within limits** âœ…

**Error Handling:**

- Automatic retry on 429 (rate limit) with exponential backoff
- Detailed error messages with status codes
- Continues processing even if one webhook fails
- **Well within limits** âœ…

### 8. **GitHub Actions Workflow**

**Trigger:**

```yaml
schedule:
  - cron: "0 16 * * *" # Daily at 16:00 UTC
workflow_dispatch: # Manual trigger
````

**Steps:**

1. **Checkout** â†’ Clone repo
2. **Setup Python** â†’ Install 3.11 + cache pip
3. **Install deps** â†’ `pip install -r requirements.txt`
4. **Run scraper** â†’ Execute with `DISCORD_WEBHOOK` secret
5. **Commit state** â†’ Push `state.json` back to repo

**Git Config:**

```bash
user.email: github-actions[bot]@users.noreply.github.com
user.name: github-actions[bot]
commit message: ğŸ¤– Update state.json [skip ci]
```

**[skip ci]** prevents infinite loop (workflow won't re-trigger on its own commits)

## âš¡ Performance Benchmarks

## âš¡ Performance Benchmarks

| Operation                       | Time        | Notes                   |
| ------------------------------- | ----------- | ----------------------- |
| Full scrape (15 category pages) | 1-3s        | Parallel async          |
| Parse 1 page                    | 5-10ms      | selectolax is fast      |
| Fetch 1 article                 | 200-400ms   | Full content extraction |
| Parse Serbian date              | <1ms        | Regex-based             |
| Send 1 Discord embed            | 50-150ms    | Webhook POST            |
| **Total run (no new articles)** | **~2-5s**   | âœ…                      |
| **Total run (10 new articles)** | **~30-50s** | âœ… (includes sorting)   |
| **Daily run (typical)**         | **~10-20s** | âœ… (1-5 new articles)   |

## ğŸ”’ Security Considerations

1. **Secrets Management**

   - `DISCORD_WEBHOOK` stored in GitHub Secrets
   - Never logged or exposed in Actions output
   - Environment variable only (not in code)

2. **Web Scraping Ethics**

   - User-Agent set: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
   - Rate limiting: 0.5s between article fetches, 2.0s between Discord posts
   - Respects server resources
   - Public academic site (no auth bypass)
   - Date filtering reduces unnecessary fetches
   - Rate limiting: 0.7s between requests
   - Respects server resources
   - Public academic site (no auth bypass)

3. **Error Handling**
   - Retries with exponential backoff on network errors
   - Automatic retry on Discord 429 (rate limit) errors
   - Graceful degradation (skip articles if parse fails)
   - Never crashes on parse errors
   - Detailed error logging with status codes

## ğŸ“Š Scalability

**Current capacity:**

- Monitors: 15 SIP categories
- Typical: 200-250 article URLs tracked
- Daily new: 5-15 articles (after date filtering)
- Growth potential: Can handle 1000+ articles in state.json

**To scale further:**

- Add more categories to `LIST_PAGES` list
- Adjust `CUTOFF_DATE` if needed
- Adjust schedule (currently daily at 16:00 UTC)
- Consider database instead of JSON (for 10,000+ articles)

## ğŸ§ª Testing Strategy

**Local testing:**

```bash
export DISCORD_WEBHOOK="..."
python watcher.py
```

**GitHub Actions testing:**

- Use `workflow_dispatch` for manual runs
- Check logs in Actions tab
- Verify `state.json` commits

**Validation:**

- âœ… No syntax errors (py_compile)
- âœ… Dependencies pinned (requirements.txt)
- âœ… Error handling on all I/O
- âœ… Graceful first-run handling
- âœ… Deduplication tested

## ğŸ”® Future Enhancements

Potential improvements:

1. **Multi-channel support**

   - Different webhooks for different sections
   - Priority channels for "VaÅ¾na obaveÅ¡tenja"

2. **Rich article previews**

   - Extract first paragraph as description
   - Add thumbnail images

3. **Advanced filtering**

   - Keyword-based routing
   - Student year filtering

4. **Analytics**

   - Track posting frequency
   - Most active sections
   - Response time metrics

5. **Database backend**
   - PostgreSQL/SQLite for 1000+ articles
   - Query historical posts
   - Better state management

## ğŸ“¦ Dependencies

| Package      | Version | Size   | Purpose            |
| ------------ | ------- | ------ | ------------------ |
| `httpx`      | 0.27.2  | ~500KB | Async HTTP client  |
| `selectolax` | 0.3.24  | ~1MB   | HTML parser (Rust) |

**Total install size:** ~1.5-2MB (extremely lightweight!)

## ğŸ¯ Design Decisions

**Why httpx over requests?**

- Async support â†’ parallel fetching
- HTTP/2 support â†’ faster
- Modern API â†’ better error handling

**Why selectolax over BeautifulSoup?**

- 5-25x faster (Rust + Modest engine)
- Lower memory usage
- CSS selectors built-in

**Why JSON over database?**

- Simplicity (single file)
- Git-friendly (trackable changes)
- No external dependencies
- Sufficient for <1000 articles

**Why GitHub Actions over external cron?**

- Free for public repos
- Built-in secrets management
- Version control integration
- Easy debugging (logs)

---

Built with â¤ï¸ for speed and reliability.
